Loss function is the representation of the optimal target. But most of the time we can not use the optimal target directly, so we choose the loss function to approach the optimal target as much as possible.
损失函数是最优化目标的一种代表，大多数情况下，我们无法直接用最优化目标，故用损失函数来替代。因此，如何选择一个损失函数，以让他和最优化目标更为接近显得极为重要。

# 1. Review paper

Ma et al., 2020, Nanjing University of Science and Technology, [Segmentation Loss Odyssey](https://arxiv.org/pdf/2005.13449.pdf)

# 2. Loss functions
This figure gives out the relations between all the loss functions.
![loss_relation](https://user-images.githubusercontent.com/42667259/90231756-e96dec80-de1b-11ea-8111-fb57bdf4974f.png)

## 2.1 Distribution-based Loss
Distribution-based loss functions aim to minimize dissimilarity between two distributions. The most fundamental function in this category is cross entropy; all
other functions are derived from cross entropy.
基于分布的损失函数旨在最小化两个分布之间的差异， 此类别中最基本的是交叉熵。 所有其他函数都可以看做是推导自交叉熵。

- Cross entropy (CE) is derived from Kullback-Leibler (KL) divergence, which is a measure of dissimilarity between two distributions. minimizing KL divergence is equivalent to minimizing CE. where g_i^c is binary indicator if class label c is the correct classification for pixel i, and s_i^c is the corresponding predicted probability.
交叉熵（CE）来自Kullback-Leibler（KL）散度，该散度是两个分布之间差异的度量。 最小化KL差异等效于最小化CE。如下图，i是每个pixel，c是分类，g_i^c是表示是否分类正确，s_i^c是分为某个类的概率

![loss_ce](https://user-images.githubusercontent.com/42667259/90253399-619ad900-de41-11ea-9da6-c7cde4260ac2.png)

- WCE, used in UNet, 2015, more details see https://github.com/senbinyu/Computer_Vision_Literatures/edit/master/3_segmentation/
WCE (Weighted cross entropy) is a common extension, with weights before g_^c in the figure.这里好理解，即把权重置于每个对应类的损失前，这样就是加权CE了。

![loss_wce](https://user-images.githubusercontent.com/42667259/90255692-eb987100-de44-11ea-901b-b6b27d7cd69d.png)

- TopK loss, Wu et al., 2016, The University of Adelaide [Bridging category-level and instance-level semantic image segmentation](https://arxiv.org/pdf/1605.06885.pdf)  
TopK loss aims to force networks to focus on hard samples during training. where t in (0; 1] is a threshold and 1{...} is the binary indicator function. In
other words, \easy" pixels, i.e., the pixels with losses below t, are dropped as they are easily classified by the current model. 
这里可以这样理解，当预测为c类时，且其概率小于某个阈值，表明这是不好预测的那些，公式中只计算这些hard samples，而把简单的直接忽略了，因此叫topK,最难的那些对loss有影响。

![loss_topK](https://user-images.githubusercontent.com/42667259/90254335-d91d3800-de42-11ea-8ec5-1d66aa73ca01.png)

- FL focal loss, used in RetinaNet, 2017, originally used for object detection, [Focal Loss for Dense Object Detection](https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)  
Focal loss [8] adapts the standard CE to deal with extreme foreground-background class imbalance, where the loss assigned to well-classified examples is reduced.
如下图所示，是easy sample的时候，s_i^c接近1，这样前面的系数(1-s_i^c)^\gamma越小，即他们的影响越小。同时，$\gamma$取得越大，其让easy sample所占比越小。反过来说，即hard sample所占的比重越大，focal到了hard sample上，也相当于一种注意力机制。

![loss_fl](https://user-images.githubusercontent.com/42667259/90255710-efc48e80-de44-11ea-8a02-0e6b8cf9dc08.png)

- DPCE, Caliva et al., 2019 University of California [Distance map loss penalty term for semantic segmentation](https://arxiv.org/pdf/1908.03679.pdf)  
Distance map penalized cross entropy loss (DPCE), weights cross entropy by distance maps which are derived from ground truth masks. It aims to guide the network’s focus towards hard-tosegment boundary regions. where D is the distance penalty term, and ◦ is the Hadamard product. Specifically, D is generated by computing distance transform of ground truth and then reverting them.
权重是通过距离图的交叉熵得出的，该距离图是指和ground truth masks的距离，距离越远则表示越难分割，则权重要变大。旨在引导网络将注意力集中在难以分割的边界区域。 其中D是距离罚分项，而◦是Hadamard乘积。 具体而言，D是通过计算与ground truth的距离变换然后将其还原而生成的。

![loss_dpce](https://user-images.githubusercontent.com/42667259/90255708-efc48e80-de44-11ea-8bc2-cdbcbafa8536.png)

## 2.2 Region-based loss
Region-based loss functions aim to minimize the mismatch or maximize the overlap regions between ground truth G and predicted segmentation S. The key element is the Dice loss. 
基于区域的loss旨在最大程度地减少失配或最大化ground truth G与预测的分割S之间的重叠区域。

- Dice loss, used in V-Net, 2016, more details see https://github.com/senbinyu/Computer_Vision_Literatures/edit/master/3_segmentation/
Dice loss can directly optimize the Dice coefficient which is the most commonly used segmentation evaluation metric. Unlike weighted cross entropy, it does not require class re-weighting for imbalanced segmentation tasks. But in reality, CE loss is more frequently used in segmentation tasks. After reviewing some papers, I found that Dice loss is hard to have convergence.
Dice loss其实是Dice参数的直接表现，后者经常用作segmentation任务中的metric。Dice loss不需要对imbalanced的例子做重新权衡。至于为何在segmentation任务中不常用，查阅资料后发现dice loss不利于反向传播，且容易造成训练时的参数振荡，不利于收敛。公式理解，dice的metric为两者相乘比上两者的平方和，越高表明越好，因此loss就在前面加上1-，这样就可以用min方法,符合loss越低越好。

![loss_dice](https://user-images.githubusercontent.com/42667259/90259253-fefa0b00-de49-11ea-956a-99b74f120fc0.png)

- IOU loss, Rahman et al., 2016, University of Manitoba, [Optimizing intersection-over-union in deep neural networks for image segmentation](http://www.cs.umanitoba.ca/~ywang/papers/isvc16.pdf)  
Similar to Dice loss, is also used to directly optimize the object category segmentation metric.
iou loss思路也和dice loss一样，也是直接用来衡量iou metric，因为iou表示的是两者的交集比上两者的并集，即减号后面部分，为了符合loss越低越好，前面加1-。

![loss_iou](https://user-images.githubusercontent.com/42667259/90261090-c3147500-de4c-11ea-9bf6-204696c10df8.png)

- Tversky loss, Salehi et al., 2017, Northeastern University [Tversky loss function for image segmentation using 3d fully convolutional deep networks](https://arxiv.org/pdf/1706.05721.pdf)  
To achieve a better trade-off between precision and recall, Tversky loss reshapes Dice loss and emphasizes false negatives. α and β are hyper-parameters which control the balance between false negatives and false positives. 
Tversky loss调整了Dice loss的结构，加上α和β来控制FN（分母的中间项）和FP（分母最后一项）之间的平衡。

![loss_tvesky](https://user-images.githubusercontent.com/42667259/90261821-ca884e00-de4d-11ea-83aa-3060b087ff83.png)

- Generalized Dice loss, Sudre et al., 2017, University College London [Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations](https://arxiv.org/pdf/1707.03237.pdf)  
Generalized Dice loss is the multi-class extension of Dice loss where the weight of each class is inversely proportional to the label frequencies. where wc = 1/
(sum g_i^c)^2 is used to provide invariance to different label set properties.
是Dice loss的多类扩展，其中每个类别的权重与标签频率成反比，wc = 1/(sum g_i^c)^2为不同的标签集属性提供不变性。

![loss_Gdice](https://user-images.githubusercontent.com/42667259/90262514-ada04a80-de4e-11ea-8ca5-effb701d0182.png)

- Focal Tversky loss, Abraham et al., 2019, Ryerson University [A novel focal tversky loss function with improved attention u-net for lesion segmentation](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759329)  
Focal Tversky loss applies the concept of focal loss to focus on hard cases with low probabilities. γ varies in the range [1, 3]
Focal Tversky loss在Tversky loss基础上，直接加系数，用来集中到hard cases上，类似于focal loss的思想。

![loss_ftl](https://user-images.githubusercontent.com/42667259/90263606-2bb12100-de50-11ea-99ef-fe75f18405cc.png)

- Asymmetric similarity loss, Hashemi et al., 2018, Harvard Medical School, [Asymmetric loss functions and deep densely-connected networks for highly-imbalanced medical image segmentation: Application to multiple sclerosis lesion detection](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573779)  
Asymmetric similarity loss introduces a weighting parameter β to better adjust the weights of false positives and false negatives. It is also a special case of Tversky loss when α + β = 1 in previous Tversky loss.
Asymmetric similarity los引入了加权参数β，以更好地调整假阳性和假阴性的权重。当先前Tversky损失中的α+β= 1时，这也是Tversky损失的一种特殊情况。

![loss_asl](https://user-images.githubusercontent.com/42667259/90264223-1092e100-de51-11ea-92a0-6e8529aa1f3d.png)

- Penalty loss, Yang et al., 2019, University of Ulsan College of Medicine, [Major Vessel Segmentation on X-ray Coronary Angiography using Deep Networks with a Novel Penalty Loss Function](https://openreview.net/pdf?id=H1lTh8unKN)  
Penalty loss penalizes false negatives and false positives in generalized Dice loss LGD. k is a non-negative penalty coefficient. When k = 0, LpGD is equivalent
to generalized Dice loss. When k > 0, LpGD gives additional weights to false positives and false negatives. 
Penalty loss惩罚generalized Dice loss LGD中的FN和FP。 k是非负惩罚系数,当k = 0时，LpGD等效generalized Dice loss。当k> 0时，LpGD为FP和FN赋予额外的权重。

![loss_penalty](https://user-images.githubusercontent.com/42667259/90265107-6320cd00-de52-11ea-9445-361f8c6405bb.png)

## 2.3 Boundary-based Loss
Boundary-based loss, a new type of loss function, aims to minimize the distance between ground truth and predicted segmentation.

- Boundary (BD) loss
To compute the distance Dist(pG; pS) between two boundaries in a differentiable way, boundary loss uses integrals over the boundary instead of unbalanced integrals over regions to mitigate the difficulties of highly unbalanced segmentation. DG(p) is the distance map of ground truth, s(p) is binary indicator function.
为了以可微分的方式计算两个边界之间的距离Dist（pG; pS），边界损耗使用边界上的积分而不是不平衡的对区域进行积分以减轻高度不平衡的细分带来的困难。DG(p)是与ground truth的距离, s(p)是双值函数，φG = −DG(q) if q 2 G, and φG = DG(q) otherwise。作者在文章是用Dice loss + BD loss结合，前期Dice loss比重较高，后期BD loss比重变高，可以更好的处理边界问题。

![loss_bd](https://user-images.githubusercontent.com/42667259/90266686-a845fe80-de54-11ea-95ff-af2b52e8eb37.png)

- Hausdorff Distance (HD) loss, Karimi et al., 2019, TheUniversity ofBritish Columbia, [Reducing the hausdorff distance in medical image segmentation with convolutional neural networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8767031)  
Since minimizing HD directly is intractable and could lead to unstable training, Karimi et al. show that it can be approximated by the distance transforms of ground truth and predicted segmentation. Further more, the network can be trained with following HD loss function1 to reduce HD. dG and dS are distance transforms of ground truth and segmentation.
由于直接将HD直接最小化是很棘手的，并且可能导致不稳定的训练，Karimi等人证明它可以通过ground truth和预测的分割的距离变换来近似。 此外，可以使用以下HD loss功能来训练网络。dG和dS是ground truth和分割的距离变换。

![loss_hd](https://user-images.githubusercontent.com/42667259/90267288-9add4400-de55-11ea-9fd7-07f86a2b3f2e.png)

- Discussion of connections between Dice loss, BD loss and HD loss
It can be found that all three loss functions aim to minimize the mismatch regions ∆M between ground truth and segmentation. The key difference among them is the weighting methods. For Dice loss, the segmentation mismatch is weighted by the sum of the number of foreground pixels in the segmentation and the number of pixels in ground truth. In BD loss, it is weighted by the distance transform map of ground truth. HD loss not only uses the distance transform map of the ground truth for weighting, but also uses the distance transform map of the segmentation.

所有三个损失函数的目的是使ground truth和segmentation的失配区域ΔM最小。 它们之间的主要区别是加权方法。 对于Dice损失，分割不匹配由分割中前景像素数与ground truth中像素数之和加权。 在BD损失中，它由ground truth的距离变换map加权。 HD损失不仅使用ground truth的距离变换map进行加权，还使用分割的距离变换map。

## 2.4 Compound loss
- Combo loss, Taghanaki et al., 2019, Simon Fraser University, [Combo loss: Handling input and output imbalance in multi-organ segmentation](https://www.sciencedirect.com/science/article/pii/S0895611118305688)   
Combo loss is the weighted sum between weighted CE and Dice loss.
Combo loss是直接的weighted CE和Dice loss的结合，用超参数alpha和1-alpha结合。

![loss_combo](https://user-images.githubusercontent.com/42667259/90268818-e264cf80-de57-11ea-96ef-39e59ab79825.png)

- Exponential Logarithmic loss (ELL), Wong et al., 2018, IBM Research, [3d segmentation with exponential logarithmic loss for highly unbalanced object sizes](https://arxiv.org/pdf/1809.00076.pdf)  
ELL makes exponential and logarithmic transforms to both Dice loss an cross entropy loss. In this way, the network can be forced to intrinsically focus more on less accurately predicted structures.
相当于在之前的基础上，加上了指数，这样就可以更加focus到不太准确的预测上，类似于focal loss的思想。

![loss_ell](https://user-images.githubusercontent.com/42667259/90268820-e2fd6600-de57-11ea-9257-9f1812fb27df.png)


How to choose the loss function? Overall, using compound loss functions is a better choice. 
那怎么选择损失函数？总的来讲，使用复合损失函数表现会相对好一些。但没有完美的loss函数，只能视具体任务而定。
